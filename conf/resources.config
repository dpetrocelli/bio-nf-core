/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Advanced resource configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    CPU, memory, time, GPU allocation for different process types
*/

process {
    
    // Default resources for all processes
    cpus   = { check_max( 1 * task.attempt, 'cpus' ) }
    memory = { check_max( 6.GB * task.attempt, 'memory' ) }
    time   = { check_max( 4.h * task.attempt, 'time' ) }
    
    // GPU support (disabled by default)
    // accelerator = 0
    
    // Resource scaling on retry
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries = 2
    
    /*
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        Resource labels - Standard nf-core classification
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    */
    
    withLabel: process_single {
        cpus   = { check_max( 1                     , 'cpus'    ) }
        memory = { check_max( 6.GB  * task.attempt , 'memory'  ) }
        time   = { check_max( 4.h   * task.attempt , 'time'    ) }
    }
    
    withLabel: process_low {
        cpus   = { check_max( 2     * task.attempt , 'cpus'    ) }
        memory = { check_max( 12.GB * task.attempt , 'memory'  ) }
        time   = { check_max( 4.h   * task.attempt , 'time'    ) }
    }
    
    withLabel: process_medium {
        cpus   = { check_max( 6     * task.attempt , 'cpus'    ) }
        memory = { check_max( 36.GB * task.attempt , 'memory'  ) }
        time   = { check_max( 8.h   * task.attempt , 'time'    ) }
    }
    
    withLabel: process_high {
        cpus   = { check_max( 12    * task.attempt , 'cpus'    ) }
        memory = { check_max( 72.GB * task.attempt , 'memory'  ) }
        time   = { check_max( 16.h  * task.attempt , 'time'    ) }
    }
    
    withLabel: process_long {
        time   = { check_max( 20.h  * task.attempt , 'time'    ) }
    }
    
    withLabel: process_high_memory {
        memory = { check_max( 200.GB * task.attempt , 'memory' ) }
    }
    
    withLabel: process_gpu {
        // GPU processes - requires NVIDIA runtime
        accelerator = 1
        memory = { check_max( 16.GB * task.attempt , 'memory'  ) }
        time   = { check_max( 8.h   * task.attempt , 'time'    ) }
        // Example: TensorFlow, PyTorch, CUDA applications
    }
    
    /*
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        Process-specific resource allocation
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    */
    
    // Light processes
    withName: 'VALIDATE_FASTA.*' {
        cpus   = 1
        memory = { check_max( 2.GB * task.attempt, 'memory' ) }
        time   = { check_max( 1.h  * task.attempt, 'time'   ) }
    }
    
    withName: 'COUNT_FASTA_HEADERS.*' {
        cpus   = 1  
        memory = { check_max( 1.GB * task.attempt, 'memory' ) }
        time   = { check_max( 30.m * task.attempt, 'time'   ) }
    }
    
    // Analysis processes - more resources
    withName: 'ANALYZE_COMPOSITION.*' {
        cpus   = { check_max( 2 * task.attempt, 'cpus' ) }
        memory = { check_max( 4.GB * task.attempt, 'memory' ) }
        time   = { check_max( 2.h * task.attempt, 'time' ) }
    }
    
    // Bioinformatics tools - variable resources
    withName: 'FASTQC.*' {
        cpus   = { check_max( 4 * task.attempt, 'cpus' ) }
        memory = { check_max( 8.GB * task.attempt, 'memory' ) }
        time   = { check_max( 2.h * task.attempt, 'time' ) }
    }
    
    // Summary processes - minimal resources
    withName: 'SUMMARIZE.*|DUMP_VERSIONS.*' {
        cpus   = 1
        memory = { check_max( 2.GB * task.attempt, 'memory' ) }
        time   = { check_max( 1.h  * task.attempt, 'time'   ) }
    }
    
    /*
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        Environment-specific configurations
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    */
}

// Resource limits for different execution environments
params {
    
    // Default limits (laptop/desktop)
    max_memory = '16.GB'
    max_cpus   = 8
    max_time   = '8.h'
    
    // GPU settings  
    enable_gpu = false
    gpu_type   = 'nvidia'  // 'nvidia' or 'amd'
    
    // Retry settings
    max_retries = 2
    error_strategy = 'retry'
}

/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Environment profiles with different resource limits
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

profiles {
    
    // Laptop/Desktop - Limited resources
    laptop {
        params.max_memory = '8.GB'
        params.max_cpus   = 4
        params.max_time   = '4.h'
        
        // Reduce all process resources by half
        process {
            withLabel: process_medium {
                cpus   = { check_max( 2 * task.attempt, 'cpus' ) }
                memory = { check_max( 8.GB * task.attempt, 'memory' ) }
            }
            withLabel: process_high {
                cpus   = { check_max( 4 * task.attempt, 'cpus' ) }
                memory = { check_max( 16.GB * task.attempt, 'memory' ) }
            }
        }
    }
    
    // Server - More resources available
    server {
        params.max_memory = '128.GB'
        params.max_cpus   = 32
        params.max_time   = '48.h'
    }
    
    // HPC Cluster - Maximum resources
    hpc {
        params.max_memory = '500.GB'
        params.max_cpus   = 64  
        params.max_time   = '168.h'  // 1 week
        
        // HPC-specific settings
        executor = 'slurm'
        queue = 'normal'
        clusterOptions = '--partition=compute --qos=normal'
    }
    
    // GPU-enabled processes
    gpu {
        params.enable_gpu = true
        
        docker {
            runOptions = '--gpus all'
        }
        
        process {
            withLabel: process_gpu {
                accelerator = 1
                queue = 'gpu'
                clusterOptions = '--gres=gpu:1'
            }
        }
    }
    
    // Cloud execution - Scalable resources
    cloud {
        params.max_memory = '200.GB'
        params.max_cpus   = 64
        params.max_time   = '72.h'
        
        // Cloud storage
        workDir = 's3://nextflow-workdir'
        
        // Auto-scaling
        process.executor = 'awsbatch'
        aws.batch.cliPath = '/usr/local/bin/aws'
    }
}

/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Resource monitoring and optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

// Enable resource monitoring
tower {
    enabled = false  // Set to true for Seqera Platform monitoring
}

// Resource usage reports - configurados en nextflow.config principal
// Los reports est√°n centralizados en el config principal para evitar conflictos